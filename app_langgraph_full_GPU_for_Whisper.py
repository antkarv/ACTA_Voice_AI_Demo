import os
import tempfile
import gradio as gr
import re
import win32com.client
import asyncio
import edge_tts
import json
import requests

from faster_whisper import WhisperModel
from ollama import chat
from dotenv import load_dotenv
load_dotenv()


# -----------------------
# CONFIG
# -----------------------
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.2")
WHISPER_SIZE = os.getenv("WHISPER_SIZE", "medium")  # tiny, small, medium, large
WHISPER_DEVICE = os.getenv("WHISPER_DEVICE", "cpu")      # "cuda" if available else "cpu"
WHISPER_COMPUTE = os.getenv("WHISPER_COMPUTE", "int8")   # "float16" often for cuda, "int8" for cpu

# Cloud LLM (Groq - OpenAI compatible)
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
GROQ_MODEL = os.getenv("LLM_MODEL_GROQ", "openai/gpt-oss-120b")
GROQ_BASE_URL = os.getenv("GROQ_BASE_URL", "https://api.groq.com/openai/v1")

DEFAULT_LLM_PROVIDER = os.getenv("LLM_PROVIDER", "ollama").lower()  # "ollama" or "groq"


#ACKS = ["ÎœÎ¬Î»Î¹ÏƒÏ„Î±â€¦", "Î‘Ï‚ Ï„Î¿ Î´Î¿ÏÎ¼Îµâ€¦", "ÎšÎ±Î»Î® ÎµÏÏÏ„Î·ÏƒÎ·â€¦", "Î’ÎµÎ²Î±Î¯Ï‰Ï‚â€¦", "Î‘Ï‚ Ï„Î¿ ÎµÎ¾Î·Î³Î®ÏƒÏ‰â€¦"]

SYSTEM_PROMPT = """Î•Î¯ÏƒÎ±Î¹ Î¼Î¹Î± ÎµÏ…Î³ÎµÎ½Î¹ÎºÎ®, ÏˆÏÏ‡ÏÎ±Î¹Î¼Î· ÎºÎ±Î¹ Î­Î¼Ï€ÎµÎ¹ÏÎ· Î²Î¿Î·Î¸ÏŒÏ‚ Ï„Î·Î»ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¹ÏÎ½.
ÎœÎ¹Î»Î¬Ï‚ Î Î‘ÎÎ¤Î‘ Î¬ÏˆÎ¿Î³Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬.

Î‘Î¥Î£Î¤Î—Î¡ÎŸÎ™ ÎšÎ‘ÎÎŸÎÎ•Î£ (Ï€Î¿Î»Ï ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒ):
- Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ ÎœÎŸÎÎŸ ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬. ÎœÎ— Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï‚ Î±Î³Î³Î»Î¹ÎºÎ­Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚ Î® Î±Î³Î³Î»Î¹ÎºÎ­Ï‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚. ÎœÎ·Î½ Î¾ÎµÏ€ÎµÏÎ½Î¬Ï‚ Ï„Î¹Ï‚ 1000 Î»Î­Î¾ÎµÎ¹Ï‚ ÏƒÏ„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·. 
- Î”ÏÏƒÎµ Î¼Î¹Î± ÏƒÏÎ½Ï„Î¿Î¼Î·, Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® Ï€ÎµÏÎ¯Î»Î·ÏˆÎ· ÏŒÏ‡Î¹ Ï€Î¬Î½Ï‰ Î±Ï€ÏŒ 1000 Î»Î­Î¾ÎµÎ¹Ï‚.
- ÎœÎ·Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï‚ ÏƒÏÏ€Ï„Ï…Î¾Î· (Ï€.Ï‡. Î±Î½Ï„Î¯ Î³Î¹'Î±Ï…Ï„ÏŒ, Î´ÏÏƒÎµ Î³Î¹Î± Î±Ï…Ï„ÏŒ).
- Î‘Î½ ÎµÎ¼Ï†Î±Î½Î¹ÏƒÏ„ÎµÎ¯ Ï„ÎµÏ‡Î½Î¹ÎºÏŒÏ‚ ÏŒÏÎ¿Ï‚, Î´ÏÏƒÎµ ÎµÎ»Î»Î·Î½Î¹ÎºÎ® ÎµÎ¾Î®Î³Î·ÏƒÎ·. (Ï€.Ï‡. Â«Î´Î¹Î±Ï†Ï‰Î½Î¯Î± Î¸Î¿ÏÏÎ²Î¿Ï…Â», Â«ÎµÎ¾Î±ÏƒÎ¸Î­Î½Î·ÏƒÎ·Â»)
- Î“Î¹Î± ÏƒÏ…Î½Ï„Î¿Î¼Î¿Î³ÏÎ±Ï†Î¯ÎµÏ‚ ÏŒÏ€Ï‰Ï‚ VDSL, Î³ÏÎ¬ÏˆÎµ: Â«V D S LÂ» (Î¼Îµ ÎºÎµÎ½Î¬), Î³Î¹Î± Î½Î± Î±ÎºÎ¿ÏÎ³ÎµÏ„Î±Î¹ ÏƒÏ‰ÏƒÏ„Î¬ ÏƒÏ„Î¿ TTS.
- ÎœÎ¯Î»Î± ÏŒÏ€Ï‰Ï‚ Î¸Î± Î¼Î¹Î»Î¿ÏÏƒÎµ Î­Î½Î±Ï‚ Î¼Î·Ï‡Î±Î½Î¹ÎºÏŒÏ‚ Ï€ÏÎ¿Ï†Î¿ÏÎ¹ÎºÎ¬: ÏƒÏÎ½Ï„Î¿Î¼ÎµÏ‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚, Ï‡Ï‰ÏÎ¯Ï‚ â€œÎ­ÎºÎ¸ÎµÏƒÎ·â€.
- ÎœÎ·Î½ Î±Î½Î±Ï†Î­ÏÎµÎ¹Ï‚ ÏŒÏ„Î¹ ÎµÎ¯ÏƒÎ±Î¹ Î¼Î¿Î½Ï„Î­Î»Î¿/AI. ÎœÎ·Î½ Î±Î½Î±Ï†Î­ÏÎµÎ¹Ï‚ Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ­Ï‚.

Î”ÎŸÎœÎ— Î‘Î Î‘ÎÎ¤Î—Î£Î—Î£:
1) ÎÎµÎºÎ¯Î½Î± Î Î‘ÎÎ¤Î‘ Î¼Îµ Î¼Î¹Î± ÏƒÏ…Î½Ï„Î¿Î¼Î· Ï†Î¹Î»Î¹ÎºÎ® ÎµÎ¹ÏƒÎ±Î³Ï‰Î³Î® Î±Î»Î»Î¬ ÏƒÎµ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÏŒ Ï„ÏŒÎ½Î¿.
2) Î”ÏÏƒÎµ Ï„Î·Î½ ÎºÏÏÎ¹Î± Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÏƒÎµ 3â€“6 ÏƒÏÎ½Ï„Î¿Î¼ÎµÏ‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚.
3) Î‘Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹, Ï€ÏÏŒÏƒÎ¸ÎµÏƒÎµ 2â€“4 bullets Î¼Îµ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ¬ ÏƒÎ·Î¼ÎµÎ¯Î±.

Î‘ÎšÎ¡Î™Î’Î•Î™Î‘ / Î‘Î’Î•Î’Î‘Î™ÎŸÎ¤Î—Î¤Î‘:
- ÎœÎ·Î½ ÎµÏ€Î¹Î½Î¿ÎµÎ¯Ï‚ ÎµÎ¼Ï€Î¿ÏÎ¹ÎºÎ­Ï‚/ÏƒÏ…Î¼Î²Î±Ï„Î¹ÎºÎ­Ï‚ ÎµÎ³Î³Ï…Î®ÏƒÎµÎ¹Ï‚ Ï€Î±ÏÏŒÏ‡Ï‰Î½.
- Î‘Î½ Î´ÎµÎ½ ÎµÎ¯ÏƒÎ±Î¹ ÏƒÎ¯Î³Î¿Ï…ÏÎ·, Ï€ÎµÏ‚ ÎºÎ±Î¸Î±ÏÎ¬: Â«Î”ÎµÎ½ ÎµÎ¯Î¼Î±Î¹ ÏƒÎ¯Î³Î¿Ï…ÏÎ· Î³Î¹Î± Î¿ÏÎ¹ÏƒÎ¼Î­Î½ÎµÏ‚ Î»ÎµÏ€Ï„Î¿Î¼Î­ÏÎµÎ¹ÎµÏ‚.Â» ÎºÎ±Î¹ Î¼ÎµÎ¯Î½Îµ ÏƒÎµ Î³ÎµÎ½Î¹ÎºÎ­Ï‚ Î±ÏÏ‡Î­Ï‚.

Î•Î™Î”Î™ÎšÎ‘ Î“Î™Î‘ VDSL:
Î•Î¾Î®Î³Î·ÏƒÎµ Î¼Îµ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ¬ Ï€Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î±: Î±Ï€ÏŒÏƒÏ„Î±ÏƒÎ· Î±Ï€ÏŒ ÎºÎ±Î¼Ï€Î¯Î½Î±, Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î± Ï‡Î±Î»ÎºÎ¿Ï, Ï€Î±ÏÎµÎ¼Î²Î¿Î»Î­Ï‚ (crosstalk),
ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ® ÎºÎ±Î»Ï‰Î´Î¯Ï‰ÏƒÎ·, Î»ÏŒÎ³Î¿Ï‚ ÏƒÎ®Î¼Î±Ï„Î¿Ï‚ Ï€ÏÎ¿Ï‚ Î¸ÏŒÏÏ…Î²Î¿ (SNR), ÎµÎ¾Î±ÏƒÎ¸Î­Î½Î·ÏƒÎ·.

VERY IMPORTANT: If you support thinking / chain-of-thought, ALWAYS use it to reason step-by-step before answering 
the question, but don't show the reasoning to the user and also don't overthink (check your available tokens) because 
the user must have a final answer anyway.
"""


CONFIDENCE_PROMPT = """Î”ÏÏƒÎµ ÎœÎŸÎÎŸ Î¼Î¯Î± Î»Î­Î¾Î· Î±Ï€ÏŒ: Î§Î‘ÎœÎ—Î›Î—, ÎœÎ•Î¤Î¡Î™Î‘, Î¥Î¨Î—Î›Î—.
Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎµ Ï„Î·Î½ ÎµÎ¼Ï€Î¹ÏƒÏ„Î¿ÏƒÏÎ½Î· Ï„Î·Ï‚ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ Ï€Î¿Ï… Î´ÏŒÎ¸Î·ÎºÎµ Î±Ï€ÏŒ Î­Î½Î±Î½ Ï„ÎµÏ‡Î½Î¹ÎºÏŒ Ï„Î·Î»ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¹ÏÎ½ Î²Î¬ÏƒÎµÎ¹ Ï„Î·Ï‚ ÎµÏÏÏ„Î·ÏƒÎ·Ï‚ Ï„Î¿Ï… Ï‡ÏÎ®ÏƒÏ„Î·.
- Î¥Î¨Î—Î›Î—: Î±Î½ Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÎµÎ¯Î½Î±Î¹ ÏƒÎ±Ï†Î®Ï‚, Î±ÎºÏÎ¹Î²Î®Ï‚ ÎºÎ±Î¹ Ï€Î»Î®ÏÎ·Ï‚.
- ÎœÎ•Î¤Î¡Î™Î‘: Î±Î½ Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÎµÎ¯Î½Î±Î¹ Î³ÎµÎ½Î¹ÎºÎ® Î® Î­Ï‡ÎµÎ¹ Î¼Î¹ÎºÏÎ­Ï‚ Î±Î½Î±ÎºÏÎ¯Î²ÎµÎ¹ÎµÏ‚.
- Î§Î‘ÎœÎ—Î›Î—: Î±Î½ Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÎµÎ¯Î½Î±Î¹ Î±ÏƒÎ±Ï†Î®Ï‚, Î±Î½Î±ÎºÏÎ¹Î²Î®Ï‚ Î® ÎµÎ»Î»Î¹Ï€Î®Ï‚.
"""


# -----------------------
# Load STT once
# -----------------------
stt_model = WhisperModel(WHISPER_SIZE, device=WHISPER_DEVICE, compute_type=WHISPER_COMPUTE)


def groq_chat(messages, *, model: str | None = None, max_tokens: int = 2000, temperature: float = 0.2) -> tuple[str, dict]:
    """
    Groq OpenAI-compatible /chat/completions.
    returns: (reply_text, usage_dict)
    """
    
    if not GROQ_API_KEY:
        return "[LLM error] GROQ_API_KEY missing", {}

    url = f"{GROQ_BASE_URL}/chat/completions"
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": model or GROQ_MODEL,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9,
    }
    print(f"[DEBUG] Groq chat call, model={model}, max_tokens={max_tokens}, temperature={temperature}")

    try:
        resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120)
        resp.raise_for_status()
        data = resp.json()
        reply = (data["choices"][0]["message"]["content"] or "").strip()
        usage = data.get("usage") or {}
        return reply, usage
    except Exception as e:
        return f"[LLM error] {e}", {}

def llm_answer(user_text: str, provider: str) -> str:
    provider = (provider or "ollama").lower().strip()

    if provider == "groq":
        reply, _usage = groq_chat(
            [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_text},
            ],
            max_tokens=2000,
            temperature=0.2,
        )
        print(f"[DEBUG] Groq reply: {reply}")
        ans = (reply or "").strip()

        # If Groq key missing or error -> return message (and keep demo alive)
        if ans.startswith("[LLM error]"):
            return ans

        # Greek-only guard (same idea as your Ollama guard)
        latin = sum(ch.isascii() and ch.isalpha() for ch in ans)
        if latin > 20:
            reply2, _ = groq_chat(
                [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": f"ÎÎ±Î½Î±Î³ÏÎ¬ÏˆÎµ Ï„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÎœÎŸÎÎŸ ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬, Ï‡Ï‰ÏÎ¯Ï‚ ÎºÎ±Î¸ÏŒÎ»Î¿Ï… Î±Î³Î³Î»Î¹ÎºÎ­Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚ ÎµÎºÏ„ÏŒÏ‚ Î±Ï€ÏŒ Ï„Î¹Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚ Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Ï„ÎµÏ‡Î½Î¹ÎºÎ¿Î¯ ÏŒÏÎ¿Î¹:\n\n{ans}"},
                ],
                max_tokens=2000,
                temperature=0.15,
            )
            if reply2 and not reply2.startswith("[LLM error]"):
                ans = reply2.strip()
        return ans

    # default: ollama
    ollama_reply = ollama_answer(user_text)
    print(f"[DEBUG] Ollama reply: {ollama_reply}")
    return ollama_reply


def llm_confidence(user_text: str, answer_text: str, provider: str) -> str:
    provider = (provider or "ollama").lower().strip()

    if provider == "groq":
        reply, _usage = groq_chat(
            [
                {"role": "system", "content": CONFIDENCE_PROMPT},
                {"role": "user", "content": f"Î•ÏÏÏ„Î·ÏƒÎ· Ï‡ÏÎ®ÏƒÏ„Î·:\n{user_text}\n\nÎ‘Ï€Î¬Î½Ï„Î·ÏƒÎ·:\n{answer_text}"},
            ],
            max_tokens=220,
            temperature=0.0,
        )
        print(f"[DEBUG] Groq confidence reply: {reply}")
        if not reply or reply.startswith("[LLM error]"):
            return "Î§Î±Î¼Î·Î»Î®"

        label = reply.strip().upper()
        if "Î¥Î¨Î—" in label:
            return "Î¥ÏˆÎ·Î»Î®"
        if "ÎœÎ•Î¤Î¡" in label:
            return "ÎœÎ­Ï„ÏÎ¹Î±"
        return "Î§Î±Î¼Î·Î»Î®"

    # default: ollama
    ollama_confidence_reply = ollama_confidence(user_text, answer_text)
    print(f"[DEBUG] Ollama confidence reply: {ollama_confidence_reply}")
    return ollama_confidence_reply


def transcribe_audio(audio_path: str) -> str:
    segments, _info = stt_model.transcribe(
        audio_path,
        language="el",
        task="transcribe",
        vad_filter=True,
        vad_parameters=dict(min_silence_duration_ms=500),
        beam_size=5,
        best_of=5,
    )
    text = " ".join(seg.text.strip() for seg in segments).strip()
    return text


def ollama_answer(user_text: str) -> str:
    print(f"[DEBUG] ollama_answer (model={OLLAMA_MODEL})")
    resp = chat(
        model=OLLAMA_MODEL,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_text},
        ],
        options={
            "temperature": 0.15,
            "top_p": 0.9,
            "repeat_penalty": 1.1,
            "num_predict": 20000,   # keeps answers short for voice
        },
    )
    ans = (resp["message"]["content"] or "").strip()
    # very simple guard: if too much Latin text, ask model to rephrase Greek-only
    latin = sum(ch.isascii() and ch.isalpha() for ch in ans)
    if latin > 20:
        resp2 = chat(
            model=OLLAMA_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": f"ÎÎ±Î½Î±Î³ÏÎ¬ÏˆÎµ Ï„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÎœÎŸÎÎŸ ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬, Ï‡Ï‰ÏÎ¯Ï‚ ÎºÎ±Î¸ÏŒÎ»Î¿Ï… Î±Î³Î³Î»Î¹ÎºÎ­Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚ ÎµÎºÏ„ÏŒÏ‚ Î±Ï€ÏŒ Ï„Î¹Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚ Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Ï„ÎµÏ‡Î½Î¹ÎºÎ¿Î¯ ÏŒÏÎ¿Î¹::\n\n{ans}"},
            ],
            options={"temperature": 0.15, "top_p": 0.9, "repeat_penalty": 1.1, "num_predict": 2000, },
            think=True
        )
        ans = (resp2["message"]["content"] or "").strip()

    return ans


def ollama_confidence(user_text: str, answer_text: str) -> str:
    print(f"[DEBUG] ollama_confidence (model={OLLAMA_MODEL})")
    resp = chat(
        model=OLLAMA_MODEL,
        messages=[
            {"role": "system", "content": CONFIDENCE_PROMPT},
            {"role": "user", "content": f"Î•ÏÏÏ„Î·ÏƒÎ· Ï‡ÏÎ®ÏƒÏ„Î·:\n{user_text}\n\nÎ‘Ï€Î¬Î½Ï„Î·ÏƒÎ·:\n{answer_text}\n\nÎœÎŸÎÎŸ Î· Î»Î­Î¾Î·:"},
        ],
        options={"temperature": 0.0, "top_p": 0.9, "repeat_penalty": 1.1, "num_predict": 20000, },
        think=True
    )
    label = (resp["message"]["content"] or "").strip().upper()
    if "Î¥Î¨Î—" in label:
        return "Î¥ÏˆÎ·Î»Î®"
    if "ÎœÎ•Î¤Î¡" in label:
        return "ÎœÎ­Ï„ÏÎ¹Î±"
    return "Î§Î±Î¼Î·Î»Î®"


def tts_sapi_to_wav(text: str) -> str:
    """
    Windows SAPI TTS -> WAV file (no external binaries).
    Tries to pick a Greek female voice if available.
    """
    fd, out_wav = tempfile.mkstemp(suffix=".wav")
    os.close(fd)

    # Clean text a bit for TTS
    t = (text or "").strip()
    t = re.sub(r"\s+", " ", t)

    speaker = win32com.client.Dispatch("SAPI.SpVoice")

    # Try to select a Greek voice (prefer female when possible)
    # NOTE: Available voices depend on installed Windows language packs.
    voices = speaker.GetVoices()
    # Force Zira (female) if available
    for i in range(voices.Count):
        v = voices.Item(i)
        if "zira" in (v.GetDescription() or "").lower():
            speaker.Voice = v
            break

    selected = None

    for i in range(voices.Count):
        v = voices.Item(i)
        desc = (v.GetDescription() or "").lower()
        # heuristics: greek + (female if mentioned)
        if "greek" in desc or "ÎµÎ»Î»Î·Î½" in desc or "el-gr" in desc:
            selected = v
            if "female" in desc or "woman" in desc or "Î³Ï…Î½" in desc:
                break

    if selected is not None:
        speaker.Voice = selected

    # Output to WAV file
    stream = win32com.client.Dispatch("SAPI.SpFileStream")
    # 3 = SSFMCreateForWrite
    stream.Open(out_wav, 3)
    speaker.AudioOutputStream = stream

    speaker.Speak(t)

    stream.Close()
    speaker.AudioOutputStream = None

    return out_wav


EDGE_TTS_VOICE = os.getenv("EDGE_TTS_VOICE", "el-GR-AthinaNeural")


def tts_edge_to_wav(text: str) -> str:
    """
    Edge Neural TTS (Greek) -> WAV
    No API key, online service.
    """
    fd, out_wav = tempfile.mkstemp(suffix=".wav")
    os.close(fd)

    async def _run():
        communicate = edge_tts.Communicate(
            text=(text or "").strip(),
            voice=EDGE_TTS_VOICE,
            rate="+0%",
            volume="+0%"
        )
        await communicate.save(out_wav)

    asyncio.run(_run())
    return out_wav

def tts_to_wav(text: str) -> str:
    """
    Primary: Edge TTS (Greek neural)
    Fallback: Windows SAPI (female)
    """
    try:
        return tts_edge_to_wav(text)
    except Exception as e:
        print("[WARN] Edge TTS failed, falling back to SAPI:", e)
        return tts_sapi_to_wav(text)



# ----------------------------
# LangGraph minimal integration (linear voice pipeline, no streaming)
# ----------------------------
from typing import TypedDict, Optional
from langgraph.graph import StateGraph, END

class VoiceState(TypedDict, total=False):
    audio_path: Optional[str]
    provider: str
    transparency: bool
    user_text: str
    answer_text: str
    confidence: str
    out_wav: Optional[str]

def stt_node(state: VoiceState) -> VoiceState:
    audio_path = state.get("audio_path")
    state["user_text"] = transcribe_audio(audio_path) if audio_path else ""
    return state

def answer_node(state: VoiceState) -> VoiceState:
    provider = state.get("provider", "groq")
    user_text = state.get("user_text", "")
    state["answer_text"] = llm_answer(user_text, provider)
    return state

def confidence_node(state: VoiceState) -> VoiceState:
    provider = state.get("provider", "groq")
    user_text = state.get("user_text", "")
    answer_text = state.get("answer_text", "")
    conf = llm_confidence(user_text, answer_text, provider)
    state["confidence"] = (conf or "").strip()

    # If confidence low, strongly encourage explicit uncertainty at the top (without rewriting everything)
    if state["confidence"] == "Î§Î±Î¼Î·Î»Î®" and "Î”ÎµÎ½ ÎµÎ¯Î¼Î±Î¹ ÏƒÎ¯Î³Î¿Ï…ÏÎ·" not in answer_text:
        state["answer_text"] = "Î”ÎµÎ½ ÎµÎ¯Î¼Î±Î¹ ÏƒÎ¯Î³Î¿Ï…ÏÎ· 100% â€” Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï‡ÏÎµÎ¹Î¬Î¶Î¿Î½Ï„Î±Î¹ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚.\n\n" + answer_text
    return state

def tts_node(state: VoiceState) -> VoiceState:
    answer_text = state.get("answer_text", "")
    state["out_wav"] = tts_to_wav(answer_text) if answer_text else None
    return state

_voice_graph = StateGraph(VoiceState)
_voice_graph.add_node("stt", stt_node)
_voice_graph.add_node("answer", answer_node)
_voice_graph.add_node("confidence", confidence_node)
_voice_graph.add_node("tts", tts_node)
_voice_graph.set_entry_point("stt")
_voice_graph.add_edge("stt", "answer")
_voice_graph.add_edge("answer", "confidence")
_voice_graph.add_edge("confidence", "tts")
_voice_graph.add_edge("tts", END)

VOICE_APP = _voice_graph.compile()


def run_pipeline(audio, transparency: bool, provider: str):
    """
    Non-streaming pipeline (Option A):
    STT -> LLM answer -> confidence -> TTS, executed via LangGraph.
    Gradio audio input is configured with type='filepath' in the UI.
    """
    if not audio:
        return "Î£Ï†Î¬Î»Î¼Î±: Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î®Ï‡Î¿Ï‚.", "", "", "Î§Î±Î¼Î·Î»Î®", None

    init_state: VoiceState = {
        "audio_path": audio,
        "provider": provider,
        "transparency": bool(transparency),
    }

    final_state: VoiceState = VOICE_APP.invoke(init_state)

    user_text = final_state.get("user_text", "")
    answer_text = final_state.get("answer_text", "")
    conf = final_state.get("confidence", "")
    out_wav = final_state.get("out_wav", None)

    # Preserve your UI behavior: hide transcript/answer/confidence when transparency is off
    stt_text = user_text if transparency else ""
    ai_text = answer_text if transparency else ""
    conf_text = conf if transparency else ""

    status = "ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ."
    # If LLM error, expose it in status and skip TTS
    if isinstance(answer_text, str) and answer_text.startswith("[LLM error]"):
        status = f"Î£Ï†Î¬Î»Î¼Î± LLM ({provider}): {answer_text}"
        out_wav = None

    if not user_text.strip():
        status = "Î”ÎµÎ½ ÎºÎ±Ï„Î¬Î»Î±Î²Î± ÎºÎ±Î¸Î±ÏÎ¬. Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Î¾Î±Î½Î¬."
        return status, "", "", "Î§Î±Î¼Î·Î»Î®", None

    return status, stt_text, ai_text, conf_text, out_wav

with gr.Blocks(title="ACTA Voice AI Demo") as demo:
    gr.Markdown("# ACTA Voice AI Demo (Greek)\n### â€¢ Female voice â€¢ Ollama or Groq (live switch)")

    with gr.Row():
        provider_dd = gr.Dropdown(
        label="LLM Provider",
        choices=["ollama", "groq"],
        value=DEFAULT_LLM_PROVIDER if DEFAULT_LLM_PROVIDER in ["ollama", "groq"] else "ollama",
        interactive=True,
        )
        transparency = gr.Checkbox(label="Î”Î¹Î±Ï†Î¬Î½ÎµÎ¹Î± (Î³Î¹Î± engineers)", value=False)


    with gr.Row():
        audio_in = gr.Audio(
            sources=["microphone"],
            type="filepath",
            label="ğŸ¤ Î Î¬Ï„Î± record, Î¼Î¯Î»Î± ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬, ÎºÎ±Î¹ ÏƒÏ„Î±Î¼Î¬Ï„Î±",
        )

    btn = gr.Button("â–¶ï¸ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· (STT â†’ LLM â†’ TTS)", variant="primary")
    status = gr.Textbox(label="ÎšÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·", value="ÎˆÏ„Î¿Î¹Î¼Î¿.", interactive=False)

    # Hidden panel (B)
    with gr.Accordion("Î Î¯Î½Î±ÎºÎ±Ï‚ Î”Î¹Î±Ï†Î¬Î½ÎµÎ¹Î±Ï‚ (Transcript / Answer / Confidence)", open=False):
        stt_text = gr.Textbox(label="Transcript (STT)", lines=3)
        ai_text = gr.Textbox(label="AI Answer (Text)", lines=8)
        conf = gr.Textbox(label="Confidence", interactive=False)

    audio_out = gr.Audio(label="ğŸ”Š AI Voice Output", type="filepath")
    clear_btn = gr.Button("ğŸ§¹ ÎÎ­Î± ÎµÏÏÏ„Î·ÏƒÎ· (ÎšÎ±Î¸Î¬ÏÎ¹ÏƒÎµ)")
    clear_btn.click(
        fn=lambda: (None, "ÎˆÏ„Î¿Î¹Î¼Î¿.", "", "", "", None),
        inputs=[],
        outputs=[audio_in, status, stt_text, ai_text, conf, audio_out],
    )


    btn.click(
        fn=run_pipeline,
        inputs=[audio_in, transparency, provider_dd],
        outputs=[status, stt_text, ai_text, conf, audio_out],
    )

#demo.queue().launch(server_name="127.0.0.1", server_port=7860)
if __name__ == "__main__":
    demo.queue().launch(server_name="127.0.0.1", server_port=7860, share=True)

